{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict as dd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up teams to iterate through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = ['ARI', 'ATL', 'BAL', 'BOS', 'CHC', 'CHW', 'CIN', 'CLE', 'COL', 'DET', 'HOU', 'KCR',  'LAA', 'LAD', 'MIA', 'MIL', 'MIN', 'NYM', 'NYY', 'OAK', 'PHI', 'PIT', 'SDP', 'SEA', 'SFG', 'STL', 'TBR', 'TEX', 'TOR', 'WSN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseball Reference Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.baseball-reference.com/teams/ARI/2019-schedule-scores.shtml' \n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html5lib\")\n",
    "table = soup.find(class_ = \"table_wrapper\", id = \"all_team_schedule\")\n",
    "\n",
    "headers = table.find_all('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = []\n",
    "iters = 0\n",
    "\n",
    "for i in headers:\n",
    "    if iters < 22:\n",
    "        col_names.append(i.get(\"data-stat\"))\n",
    "        iters +=1\n",
    "    else:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sched(team):\n",
    "    url = 'https://www.baseball-reference.com/teams/'+team+'/2019-schedule-scores.shtml' \n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html5lib\")\n",
    "    table = soup.find(class_ = \"table_wrapper\", id = \"all_team_schedule\")\n",
    "\n",
    "    data = table.find_all('tr')\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for i in data:\n",
    "        cols = i.find_all('td')\n",
    "        cols2 = [ele.text.strip() for ele in cols]\n",
    "        if cols2 != []:\n",
    "            rows.append(cols2)\n",
    "            \n",
    "    counter = 0\n",
    "    for i in rows:\n",
    "        i.insert(0,counter)\n",
    "        counter += 1\n",
    "        \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for i in teams:\n",
    "    total.append(get_sched(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_games = []\n",
    "for i in total:\n",
    "    for game in i:\n",
    "         all_games.append(game)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = pd.DataFrame(all_games, columns = col_names)\n",
    "games_df['team_game'] = games_df['team_game'] + 1\n",
    "games_df['date'] = games_df['date_game'].str.rsplit(', ').str[-1] +', 2019'\n",
    "games_df['DOW'] = games_df['date_game'].str.rsplit(', ').str[0]\n",
    "\n",
    "games_df['date2'] = pd.to_datetime(games_df['date'], errors='coerce').astype(str)\n",
    "games_df = games_df.drop('date', axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data Scrape (Weather Undergound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv('/Users/philliprichardson/Metis/Module 2/links.csv')\n",
    "\n",
    "w_col_headers = ['City','Month', 'Day', 'max_temp', 'avg_temp', 'min_temp', 'max_dew', 'avg_dew', 'min_dew', 'max_humid', 'avg_humid', 'min_humid', 'max_wind', 'avg_wind','min_wind', 'max_pressure', 'avg_pressure', 'min_pressure', 'precip']\n",
    "months = ['2019-3', '2019-4', '2019-5', '2019-6', '2019-7', '2019-8', '2019-9']\n",
    "urls = urls[['Tm','Base']]\n",
    "urls['Base'] = urls['Base'].apply(lambda x: x[:-6])\n",
    "links = urls['Base'].tolist()\n",
    "tms = urls['Tm'].tolist()\n",
    "\n",
    "weather_data = pd.DataFrame(columns = w_col_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operationalizing the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    \n",
    "def render_page(url):\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    r = driver.page_source\n",
    "    driver.quit()\n",
    "    return r\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_weather(url, city_name):\n",
    "    r = render_page(url)\n",
    "    soup = BeautifulSoup(r, \"html5lib\")\n",
    "\n",
    "    table = soup.find('lib-city-history-observation').find('tbody')\n",
    "    \n",
    "\n",
    "    weather = []\n",
    "\n",
    "    for r in table.find_all('tr', class_='ng-star-inserted'):\n",
    "        for i in r.find_all('td', class_='ng-star-inserted'):\n",
    "            td = i.text\n",
    "            td = td.strip('  ')\n",
    "            weather.append(td)\n",
    "\n",
    "    month = weather.pop(0)\n",
    "\n",
    "    weather.append( 'end')\n",
    "    weather.append( 'end')\n",
    "  \n",
    "\n",
    "    weath_row = []\n",
    "    w_data = []\n",
    "    keys = 1\n",
    "\n",
    "    for idx, elem in enumerate(weather):\n",
    "        \n",
    "        if elem.replace(\".\",\"\",1).replace(\"-\", \"\", 1).isdigit() == True:\n",
    "            w_data.append(elem)\n",
    "        elif elem.replace(\".\",\"\",1).replace(\"-\", \"\", 1).isdigit() == False and weather[idx - 1].replace(\".\",\"\",1).replace(\"-\", \"\", 1).isdigit() == True:\n",
    "            weath_row.append(w_data.copy())\n",
    "            keys += 1\n",
    "            w_data.clear()\n",
    "        else: \n",
    "            pass\n",
    "        \n",
    "    day_chunks = []\n",
    "\n",
    "    def divide_chunks(l, n):\n",
    "        for i in range(0, len(l), n): \n",
    "            yield l[i:i + n]\n",
    "\n",
    "\n",
    "    days = len(weath_row[0])\n",
    "\n",
    "    city = [city_name] * days\n",
    "    month_ = [month] * days\n",
    "\n",
    "    for col in weath_row:\n",
    "        day_chunks.append(divide_chunks(col, days))\n",
    "\n",
    "    w_cols = [city, month_]\n",
    "\n",
    "    for rec in day_chunks:\n",
    "        for j in list(rec):\n",
    "            w_cols.append(j)\n",
    "\n",
    "    arr = np.array(w_cols)\n",
    "\n",
    "    return pd.DataFrame(arr.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually iterating due to page crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_dfs = []\n",
    "# Manually Iterating to prevent overloading\n",
    "\n",
    "#0-29\n",
    "ctr = 0\n",
    "\n",
    "input_team = urls.iloc[ctr]['Tm']\n",
    "curr_url = urls.iloc[ctr]['Base']\n",
    "\n",
    "for i in months:\n",
    "    input_url = curr_url+i\n",
    "    weather_dfs.append(month_weather(input_url, input_team))\n",
    "    with open('weather.pkl', 'wb') as f:\n",
    "        pickle.dump(weather_dfs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Partial Scrapes indeces\n",
    "\n",
    "remove_idx = []\n",
    "\n",
    "for i, e in enumerate(weather_dfs):\n",
    "    if e.shape[1] != 19:\n",
    "        remove_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing partial scrapes from the list of all scrapes\n",
    "\n",
    "for index in sorted(remove_idx, reverse=True):\n",
    "    del weather_dfs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in weather_dfs:\n",
    "    i.columns = w_col_headers\n",
    "\n",
    "full_wd = weather_data.append(weather_dfs)\n",
    "\n",
    "full_wd = full_wd[['City', 'Month', 'Day', 'max_temp', 'avg_temp', 'precip']]\n",
    "\n",
    "full_wd.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging weather and baseball data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wd['Date'] = pd.to_datetime(full_wd['Month'] + ' ' + full_wd['Day'] +', ' + '2019').astype(str)\n",
    "\n",
    "\n",
    "all_data = games_df.merge(full_wd, left_on = ['team_ID', 'date2'], right_on = ['City', 'Date'])\n",
    "\n",
    "keep = ['team_game', 'team_ID', 'homeORvis', 'opp_ID', 'win_loss_result', 'R', 'RA', 'extra_innings', 'win_loss_record', 'games_back', 'day_or_night', 'attendance', 'win_loss_streak', 'reschedule', 'DOW', 'max_temp', 'avg_temp', 'precip', 'Date']\n",
    "all_data = all_data[keep]\n",
    "\n",
    "all_data.to_csv(r'/Users/philliprichardson/Metis/Module 2/modelbase.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
